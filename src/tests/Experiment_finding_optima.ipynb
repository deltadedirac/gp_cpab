{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc,sys\n",
    "import pdb\n",
    "\n",
    "#pdb.set_trace()\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os, mlflow, mlflow.pytorch\n",
    "import __init__\n",
    "\n",
    "from transformation.gp_cpab import gp_cpab\n",
    "from transformation.configManager import configManager\n",
    "from extra.utilities import *\n",
    "from extra.dataLoaderDiffeo import *\n",
    "from extra.LossFunctionsAlternatives import LossFunctionsAlternatives\n",
    "from extra.Automatic_Report import Generate_Automatic_Report\n",
    "from transformation.libcpab.libcpab.pytorch.interpolation import interpolate1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse; import sys; sys.argv=['']; del sys\n",
    "\n",
    "def argparser():\n",
    "    \"\"\" Argument parser for the main script \"\"\"\n",
    "    parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "    # CPAB Setup for Gaussian Process Interpolation\n",
    "    gpsetup = parser.add_argument_group('GPSetup')\n",
    "    gpsetup.add_argument('--Task', type=int, default = 5, help='Amount of channels in multitask-gp estimator')\n",
    "    #gpsetup.add_argument('--Initialization', type=float, default = 0.25, help='multitaks-gp initialization')\n",
    "    gpsetup.add_argument('--Initialization', type=list, default = [0.0, 0.0, 0.33, 0.33, 0.33], help='multitaks-gp initialization') #[0.0, 0.33, 0.33, 0.33] #[0.5, 0.5]\n",
    "    gpsetup.add_argument('--Lengthscale', type=float, default = 0.25, help='lengthscale on square-exponential kernel') # *** 0.5 *** #0.25, #0.5 + init gives better for 2 gaps case, # 1.25\n",
    "    gpsetup.add_argument('--noise_constraint', type=list, default = [0,0.0004], help='noise constrain') #16\n",
    "    gpsetup.add_argument('--Option', type=str, default = 'multitask', help='noise constrain') #16\n",
    "\n",
    "    # Paths to use\n",
    "    paths = parser.add_argument_group('Paths')\n",
    "    paths.add_argument('--path_orig', type=str, default=\"../../data/orig_3aa.fasta\", help='original sequence to deform')    #orig_3aa.fasta orig_3aag.fasta\n",
    "    paths.add_argument('--path_target', type=str, default=\"../../data/ref_3aa2g.aln\", help='Reference sequence as my desired output') #ref_3aa3g.aln\n",
    "    paths.add_argument('--path_preexist_linear', type=str, default=\"../../models/CPABlinear4.pth\", help='prebuilt model using linear case') \n",
    "    paths.add_argument('--path_preexist_gp', type=str, default=\"../../models/CPABGPB4.pth\", help='prebuilt model using gp case')\n",
    "    paths.add_argument('--path_automated_report', type=str, default=\"../../Results\", help='path to save automatic report') \n",
    "    paths.add_argument('--logdir', type=str, default=\"../../Results\", help='where to store results')\n",
    "    \n",
    "    # CPAB features\n",
    "    cpab = parser.add_argument_group('CPAB')\n",
    "    cpab.add_argument('--device', type=str, default=\"cpu\", help='device')\n",
    "    cpab.add_argument('--modeflag', type=str, default=\"1D\", help='dimensionality of tesselation')\n",
    "    cpab.add_argument('--window_grid', type=int, default=4, help='number of tesselation cells') #6\n",
    "    cpab.add_argument('--channels', type=int, default=4, help='amount of channels for estimation --deprecated')\n",
    "    cpab.add_argument('--interpolation_type', type=str, default=\"GP\", help='type of interpolation between maps') \n",
    "\n",
    "    # Optimization\n",
    "    opt = parser.add_argument_group('Optimization')\n",
    "    opt.add_argument('--lr', type=float, default=0.001, help='learning rate') #0.01, # 0.00001 just for testing the behavior for non init thetas\n",
    "    opt.add_argument('--weight_decay', type=float, default=0.0004, help='L2 regularization')\n",
    "    opt.add_argument('--maxiter', type=int, default=500, help='max number of epochs') #500 iters or less\n",
    "    opt.add_argument('--lossfunctmetric', type=str, default='Soft_Label_KLD', help='initial loss function') # 'JSD','Soft_Label_KLD', 'SKL'\n",
    "\n",
    "    # LogosPlot\n",
    "    logos = parser.add_argument_group('logos')\n",
    "    logos.add_argument('--scale_factor', type=int, default=10, help='scale factor')\n",
    "    logos.add_argument('--mode', type=str, default='see_train', help='mode') \n",
    "\n",
    "    # Parse and return\n",
    "    args = parser.parse_args()\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Ok\n"
     ]
    }
   ],
   "source": [
    "print(__package__)\n",
    "args = argparser()\n",
    "'''---------------------------------------------------------'''\n",
    "std = configManager(args)\n",
    "\n",
    "\n",
    "\n",
    "device = std.parserinfo('device')\n",
    "modeflag = std.parserinfo('modeflag')\n",
    "window_grid = std.parserinfo('window_grid')\n",
    "channels = std.parserinfo('channels')\n",
    "option = std.parserinfo('Option')\n",
    "lossmetric = std.parserinfo('lossfunctmetric')\n",
    "\n",
    "path = std.parserinfo('path_orig')\n",
    "path_MSA_test = std.parserinfo('path_target')#PathMSAref4t\n",
    "path_preexist_model = std.parserinfo('path_preexist_linear')\n",
    "path_preexist_modelGP = std.parserinfo('path_preexist_gp')\n",
    "\n",
    "pathX123 = std.parserinfo('path_automated_report')\n",
    "indexlogolinear = '_LI_3aa9g_padd'\n",
    "indexlogoGP = '_GP_3aa9g_padd'\n",
    "indexoutputT = 'debugging_cpab_domains.txt' #'results_3aa_Inverse_direct.txt'\n",
    "#self.config, self.constrain, self.tasks, self.interpolation_type, self.option \n",
    "gp_params = std.get_config_vals(['noise_constraint','Task','interpolation_type','Option','Lengthscale','Initialization'])\n",
    "print('Ok')\n",
    "padding_criteria = 'padding_weights'  # 'padding_weights', 'none'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding_pattern_input_target(x1, ref_msa, token, alphabet_dict):\n",
    "\n",
    "    padd_replication = abs(ref_msa.shape[1] - x1.shape[1])\n",
    "    padd = torch.tensor([])\n",
    "\n",
    "    if padd_replication!=0:\n",
    "        if token == 'gap':\n",
    "            channel_factor = 1./ref_msa.shape[2]\n",
    "            padd = torch.ones(1,padd_replication,ref_msa.shape[2])*channel_factor\n",
    "        elif token == 'token':\n",
    "            if '-' in alphabet_dict: token_number, num_class = alphabet_dict['-'], len(alphabet_dict)\n",
    "            # in case there not exist the token, we have to append it as column to \n",
    "            # the target, because we are including the new symbol in the dictionary\n",
    "            else: token_number, num_class = len(alphabet_dict),len(alphabet_dict)+1; ref_msa = torch.cat( (ref_msa,torch.zeros(ref_msa.shape[0],ref_msa.shape[1],1)), 2 )\n",
    "\n",
    "            padd = F.one_hot(torch.tensor(token_number), num_class).repeat(1,padd_replication,1)#.repeat(1,padd_replication,len(alphabet_dict))\n",
    "        \n",
    "        # conditional where input is filled with padding to\n",
    "        # equal target length or backwards depending of their difference\n",
    "        if x1.shape[1] <= ref_msa.shape[1]: x1 = torch.cat(( x1 ,padd),1)\n",
    "        else: ref_msa = torch.cat(( ref_msa ,padd),1)\n",
    "        \n",
    "\n",
    "    outsize = (ref_msa.shape[1], ref_msa.shape[2])\n",
    "    padded_idx = [*range(x1.shape[1],ref_msa.shape[1])]\n",
    "    non_padded_idx = set(range(0, outsize[0])) - set(padded_idx) \n",
    "    non_padded_idx = [*non_padded_idx]\n",
    "\n",
    "    ''' TEMP SOLUTION FOR THE EXPERIMENT WITH THE MODIFICATIONS'''\n",
    "    '''---------------------------------------------------------------------------'''\n",
    "    padded_idx = non_padded_idx[(ref_msa.shape[1] - padd_replication):]; non_padded_idx =  non_padded_idx[:(ref_msa.shape[1] - padd_replication)]\n",
    "    '''---------------------------------------------------------------------------'''\n",
    "\n",
    "    padded_idx = [ padded_idx ]; non_padded_idx = [ non_padded_idx ]\n",
    "    return padded_idx, non_padded_idx, x1, ref_msa, outsize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 2 Padding_pattern_input_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmp\n",
      "final input: \n",
      " tensor([[[0., 0., 0., 1., 0.],\n",
      "         [0., 0., 1., 0., 0.],\n",
      "         [0., 0., 0., 0., 1.],\n",
      "         [1., 0., 0., 0., 0.],\n",
      "         [1., 0., 0., 0., 0.],\n",
      "         [1., 0., 0., 0., 0.],\n",
      "         [1., 0., 0., 0., 0.],\n",
      "         [1., 0., 0., 0., 0.],\n",
      "         [1., 0., 0., 0., 0.],\n",
      "         [1., 0., 0., 0., 0.]]]) \n",
      "\n",
      "final target: \n",
      " tensor([[[0., 0., 0., 1., 0.],\n",
      "         [0., 1., 0., 0., 0.],\n",
      "         [0., 1., 0., 0., 0.],\n",
      "         [0., 0., 1., 0., 0.],\n",
      "         [0., 0., 0., 0., 1.],\n",
      "         [0., 1., 0., 0., 0.],\n",
      "         [0., 1., 0., 0., 0.],\n",
      "         [0., 1., 0., 0., 0.],\n",
      "         [0., 1., 0., 0., 0.],\n",
      "         [0., 1., 0., 0., 0.]]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def fill_seqs_with_symbols(seq, padd_replication, c2i, symbol='gap'):\n",
    "    if symbol == 'gap':\n",
    "        channel_factor = 1./(seq.shape[2]-1) #1./seq.shape[2]\n",
    "        padd = torch.ones(1,padd_replication,seq.shape[2]) * channel_factor\n",
    "        padd[:,:,0]=0\n",
    "    elif symbol == 'token':\n",
    "        classes = len(c2i.keys())\n",
    "        token = F.one_hot(torch.tensor(c2i['-']), classes)\n",
    "        padd = token.repeat(1,padd_replication,1)\n",
    "    elif symbol == 'token_target':\n",
    "        classes = len(c2i.keys())\n",
    "        token = F.one_hot(torch.tensor(c2i['.']), classes)\n",
    "        padd = token.repeat(1,padd_replication,1)\n",
    "\n",
    "    return torch.cat(( seq ,padd),1)\n",
    "\n",
    "def replace_target_token_gaps(seq, c2i, token_sym):\n",
    "    token_val = c2i[token_sym]\n",
    "    val_gap = torch.ones(seq.shape[2])*1./(seq.shape[2]-1)\n",
    "    val_gap[0]=0.0\n",
    "    seq[seq.argmax(-1)==token_val] =  val_gap\n",
    "    return seq\n",
    "\n",
    "def padding_strategy_input_target(x1, ref_msa, token_x, token_target, c2i, after_bound = 4):\n",
    "\n",
    "    len_inp = x1.shape[1]\n",
    "    len_target = ref_msa.shape[1]\n",
    "    padd_replication = ref_msa.shape[1] - x1.shape[1]\n",
    "\n",
    "    # make input and target to same size\n",
    "    if padd_replication > 0:\n",
    "        x1 = fill_seqs_with_symbols(x1, abs(padd_replication), c2i, symbol=token_x)\n",
    "        outsize = len_target + after_bound\n",
    "    elif padd_replication < 0:\n",
    "        ref_msa = fill_seqs_with_symbols(ref_msa, abs(padd_replication), c2i, symbol=token_target)\n",
    "        outsize = len_inp + after_bound\n",
    "    else: outsize = len_target\n",
    "\n",
    "    # increase input and target seq size after the matching of both bound sizes\n",
    "    if '-' in c2i: \n",
    "        x1 = fill_seqs_with_symbols(x1, after_bound, c2i, symbol=token_x)\n",
    "        if token_target != 'token_target':\n",
    "            ref_msa = replace_target_token_gaps(ref_msa, c2i, '.')\n",
    "        ref_msa = fill_seqs_with_symbols(ref_msa, after_bound, c2i, symbol=token_target)    \n",
    "\n",
    "    return x1, ref_msa, (outsize,outsize)\n",
    "\n",
    "\n",
    "# Reference Information - how the alignment should looks like\n",
    "alph = ['-','.', 'L', 'Q', 'R']\n",
    "alignment, ref_msa, alphabets, c2i, i2c, i2i,seqchar = read_clustal_align_output(path_MSA_test, alphabet=alph)\n",
    "# Raw Sequences, to see if we can align the sequences somehow\n",
    "dataset_msa = datasetLoader(pathBLAT_data = path, alphabet = alphabets, enable_variable_length=True)\n",
    "\n",
    "x1 = dataset_msa.prot_space\n",
    "fill_inp = 'token'\n",
    "fill_target = 'token_target' #'gap'\n",
    "\n",
    "x1, ref_msa, outsize = padding_strategy_input_target(x1.float(), ref_msa.float(), fill_inp, fill_target, c2i, after_bound = 5)\n",
    "print('final input: \\n {0} \\n'.format(x1))\n",
    "print('final target: \\n {0} \\n'.format(ref_msa))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from extra.logomakers import logomaker_plots \n",
    "\n",
    "#def plot_logos_probs(x1_trans, alphabets, folderpath = pathfolder, name = path):  \n",
    "def plot_logos_probs(x1_trans, alphabets, **kargs): #folderpath = pathfolder, name = path):  \n",
    "    #from logomakers import logomaker_plots  \n",
    "\n",
    "\n",
    "    alphabets_logo = [ i if i!='-' else 'X' for i in alphabets] \n",
    "    x1_trans_logo_input = df_construction_aas([ x1_trans.detach().numpy() ]*500, x1_trans.shape, alphabets_logo)\n",
    "    best = logomaker_plots.plotlogos(x1_trans_logo_input[x1_trans_logo_input.columns.tolist()] )\n",
    "\n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(__package__)\n",
    "from tensorboardX import SummaryWriter\n",
    "import time, os, datetime\n",
    "# Logdir for results\n",
    "if args.logdir == '':\n",
    "    logdir = args.logdir + '/res/' + 'gp_cpab_tests' + '/' + datetime.datetime.now().strftime('%Y_%m_%d_%H_%M')\n",
    "else:\n",
    "    logdir = args.logdir + '/res/' + 'gp_cpab_tests'\n",
    "  \n",
    "writer = SummaryWriter(log_dir=logdir)\n",
    "\n",
    "def training_theta_optima(path_preexist_model, theta_est, maxiter, optimizer, T,x1, ref_msa, modeflag, msa_num, loss_vals, loss_function, inverse):\n",
    "\n",
    " \n",
    "    if os.path.isfile(path_preexist_model):\n",
    "        print (\"Loading Deformation Model\")\n",
    "        theta_est = torch.load(path_preexist_model)\n",
    "    else:\n",
    "        pb = tqdm(desc='Alignment of samples', unit='iters', total=maxiter)\n",
    "        for i in range(maxiter):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if inverse == True:\n",
    "                opt_est = -theta_est\n",
    "\n",
    "                x1_trans, sampled_data, forw_per = T.spatial_transformation(x1, ref_msa, opt_est, modeflag)\n",
    "                loss = loss_function(method = lossmetric, input = x1_trans, target = ref_msa, forw_per=forw_per) # 'JSD','Soft_Label_KLD'\n",
    "\n",
    "\n",
    "            loss_vals.append(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            #torch.save(theta_est, path_preexist_model)\n",
    "\n",
    "            pb.update()\n",
    "            pb.set_postfix({'loss': str(loss.item())})\n",
    "\n",
    "            msa_num.append(x1_trans.detach().numpy().squeeze())\n",
    "\n",
    "\n",
    "        pb.close()\n",
    "        shapes = ref_msa.squeeze().shape\n",
    "        msa_num = np.vstack(msa_num).reshape(-1, shapes[0], shapes[1])\n",
    "\n",
    "\n",
    "    return theta_est\n",
    "\n",
    "\n",
    "#writer.add_scalar('test/total_loss', test_loss, iteration)\n",
    "#writer.add_image('samples/samples', make_grid(samples.cpu(), nrow=n), \n",
    "#                                 global_step=epoch)\n",
    "\n",
    "def get_interpolated_data(data, T, outsize):\n",
    "    data1=data.unsqueeze(0)\n",
    "    grid = T.uniform_meshgrid(outsize).repeat(data1.shape[0],1,1)\n",
    "    out = interpolate1D(data1,grid,outsize)\n",
    "    return out.squeeze(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_data_and_transformations(path_MSA_test, path, gp_params, option, T, type_of_fill='token,gap', length_filling = 3, **kargs):\n",
    "\n",
    "    if 'alphabet' in kargs:\n",
    "        dict_aas = kargs['alphabet']\n",
    "    # Reference Information - how the alignment should looks like\n",
    "    alignment, ref_msa, alphabets, c2i, i2c, i2i,seqchar = read_clustal_align_output(path_MSA_test, alphabet = dict_aas)\n",
    "    # Raw Sequences, to see if we can align the sequences somehow\n",
    "    dataset_msa = datasetLoader(pathBLAT_data = path, alphabet = alphabets, enable_variable_length=True)\n",
    "\n",
    "    x1 = dataset_msa.prot_space\n",
    "    #padded_idx, non_padded_idx, x1, ref_msa, outsize = padding_pattern_input_target(x1, ref_msa, type_of_fill, c2i)\n",
    "\n",
    "    x1 = T.backend.to(x1.clone().detach(), device=device)\n",
    "    ref_msa = T.backend.to(ref_msa.clone().detach(), device=device)\n",
    "\n",
    "    fill_inp , fill_target = type_of_fill.split(',')\n",
    "    x1, ref_msa, outsize = padding_strategy_input_target(x1.float(), ref_msa.float(), fill_inp, fill_target, c2i, after_bound = length_filling)\n",
    "    padded_idx, non_padded_idx = [[]], [[]]\n",
    "\n",
    "    ''' LIKELIHOOD DEFINITIONS:'''\n",
    "    \n",
    "    if option == 'multitask':\n",
    "        '''\n",
    "        T.get_interpolation_inductive_points(x1, x1.float(), \n",
    "                                            outsize = outsize, \n",
    "                                            padded_idx = padded_idx, \n",
    "                                            non_padded_idx = non_padded_idx,\n",
    "                                            #separation_size_grid = T.params.inc[0],\n",
    "                                            padding_option = padding_criteria  )\n",
    "                                            #padding_option = 'none'   )'''\n",
    "    \n",
    "    return alphabets, c2i, i2c, i2i, dataset_msa, padded_idx, non_padded_idx, x1, ref_msa, outsize, T\n",
    "\n",
    "        \n",
    "    #return alphabets, c2i, i2c, i2i, dataset_msa, padded_idx, non_padded_idx, x1, ref_msa, outsize, T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_thetas(init=True):\n",
    "    \n",
    "    ''' FOR CASES 1,2,3,4 GAPS WITH INVERSE'''\n",
    "\n",
    "    #theta_ref = torch.autograd.Variable(torch.tensor([[-0.3,0.0011522,0.0018782, 0.2456, -0.3125]], requires_grad=True), requires_grad=True)\n",
    "    #theta_ref  = get_interpolated_data(theta_ref, T, (T.params.nC+1,T.params.nC+1))\n",
    "    ''' FOR CASES 10 GAPS WITH INVERSE'''\n",
    "    #theta_ref = torch.autograd.Variable(torch.tensor([[0.003,  -0.006, 0.003, 0.006,  -0.003]], requires_grad=True), requires_grad=True)\n",
    "    if init==True:\n",
    "        '''REFERENCE FOR TEMPORARY SOLUTION 3AA 1,2,3,4 GAPS TESS SIZE 4 CELLS'''\n",
    "        #theta_ref = torch.tensor([[-0.31, -0.06, -0.004, -0.01, 0.9875, -0.23, -0.2]]) *(-1 )\n",
    "        '''REFERENCE FOR TEMPORARY SOLUTION 3AA 1,2,3,4 GAPS TESS SIZE 4 CELLS'''\n",
    "        #theta_ref = torch.autograd.Variable(torch.tensor([[-0.3,0.0011522,0.0018782, 0.2456, -0.3125]], requires_grad=True), requires_grad=True)\n",
    "        '''REFERENCE FOR IDEAL CASE 3 AA 3 GAPS''' #***********\n",
    "        theta_ref = torch.autograd.Variable(torch.tensor([[0.0011, 0.019, -0.119, -0.015356, 0.952]],requires_grad=True), requires_grad = True )*(-1)\n",
    "        '''REFERENCE FOR IDEAL CASE 3 AA 1 GAP CASE ONE -> SEE ORIG3AAG VS REF_3AA token,token'''\n",
    "        #theta_ref = torch.autograd.Variable(torch.tensor([[-0.3,0.0011522,0.00018782, 0.002456, -0.0125]], requires_grad=True), requires_grad=True)\n",
    "        '''REFERENCE FOR IDEAL CASE 3 AA 1 GAP CASE ONE -> SEE ORIG3AA VS REF_3AA'''\n",
    "        #theta_ref = torch.autograd.Variable(torch.tensor([[-0.3,0.0011522,0.0018782, 0.2456, -0.3125]], requires_grad=True), requires_grad=True)\n",
    "        '''JUST FOR EXAMINING THE SUBOBTIMAL SOLUTION'''\n",
    "        #theta_ref = torch.autograd.Variable( torch.tensor([[ 0.1934,  0.1863, -0.0259,  0.1988,  0.1951]], requires_grad=True) ,requires_grad = True) # A)\n",
    "        #theta_ref = torch.autograd.Variable( torch.tensor([[ 0.2757,  0.3758, -0.2276,  0.3919,  0.3862]], requires_grad=True) ,requires_grad = True)  # B)\n",
    "        #theta_ref = torch.autograd.Variable( torch.tensor([[ 0.0637,  0.5615, -0.4211,  0.5806,  0.5747]], requires_grad=True) ,requires_grad = True)  # C)\n",
    "        #theta_ref = torch.autograd.Variable( torch.tensor([[-0.1374,  0.7423, -0.6146,  0.7648,  0.7556]], requires_grad=True) ,requires_grad = True)  # D)\n",
    "        \n",
    "        \n",
    "\n",
    "    else:\n",
    "        theta_ref = torch.autograd.Variable(T.identity(1, epsilon=1e-6), requires_grad=True)\n",
    "    theta_est = torch.autograd.Variable(theta_ref.clone(), requires_grad=True)\n",
    "    theta_est_GP = torch.autograd.Variable(theta_ref.clone(), requires_grad=True)\n",
    "    return theta_ref, theta_est, theta_est_GP\n",
    "\n",
    "def optimization_setup(conf, theta_est, theta_est_GP, c2i, i2c, i2i, keys = ['lr','weight_decay','maxiter']):\n",
    "    '''lr=0.01 is the best one so far for linear interpolation and gp'''\n",
    "        \n",
    "    lr, wd, maxiter = conf.get_config_vals(keys)\n",
    "    #lr = std.parserinfo('*/lr') \n",
    "    #wd = std.parserinfo('*/weight_decay')\n",
    "    #maxiter = std.parserinfo('*/maxiter')\n",
    "\n",
    "    optimizer = torch.optim.AdamW([theta_est], lr=lr) #, weight_decay=wd)\n",
    "    optimizerGP = torch.optim.AdamW([theta_est_GP], lr=lr)\n",
    "\n",
    "\n",
    "    loss_function = LossFunctionsAlternatives()\n",
    "    loss_function.get_dictionaries_to_mask_data(c2i, i2c, i2i)\n",
    "\n",
    "    return lr, wd, maxiter, optimizer, optimizerGP, loss_function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_irregular_table(cm, x_lab, y_lab,cmap='viridis'): \n",
    "    import itertools \n",
    "\n",
    "    figure, ax = plt.subplots(figsize=(8,10)) # plt.figure(figsize=(15,15))\n",
    "    if cmap=='none':\n",
    "        im = ax.imshow(cm, interpolation='None') \n",
    "    else:\n",
    "        im = ax.imshow(cm, interpolation='None', cmap='viridis') \n",
    "    ax.set_title(\"value matrix\") \n",
    "    #ax.set_colorbar() \n",
    "    tick_marks_x = np.arange(0.5,len(x_lab)+0.5) \n",
    "    tick_marks_y = np.arange(0.5,len(y_lab)+0.5) \n",
    "\n",
    "    ax.set_xticks(tick_marks_x, x_lab, rotation=45) \n",
    "    ax.set_yticks(tick_marks_y, y_lab)\n",
    "    \n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):   \n",
    "        #color = \"white\" if cm[i, j] > threshold else \"black\"\n",
    "        color = \"white\"\n",
    "        ax.text(j, i, cm[i, j],  ha=\"center\", va=\"center\", color=\"w\")  \n",
    "    \n",
    "    ax.set_ylabel('True label') \n",
    "    ax.set_xlabel('Predicted label') \n",
    "    \n",
    "    ax.grid()\n",
    "    plt.close()\n",
    "    plt.cla()\n",
    "    plt.clf()\n",
    "    return figure\n",
    "    '''\n",
    "    plt.tight_layout() \n",
    "    return figure\n",
    "    '''\n",
    "\n",
    "def heatmap_from_tensor(data, alphabet, title):\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    figure, ax = plt.subplots(figsize=(8,10))\n",
    "    #ax = plt.axes()\n",
    "    px = pd.DataFrame(data, columns=alphabet)\n",
    "    ff=sns.heatmap(px, linewidth=1, linecolor='w', annot=data, ax = ax)\n",
    "    ax.set_title(title)\n",
    "    plt.close()\n",
    "    plt.cla()\n",
    "    plt.clf()\n",
    "    return figure\n",
    "    #plt.show()\n",
    "\n",
    "#rr=heatmap_from_tensor(invtrans.detach().numpy()[0],['-','L','Q','R'], 'tmp')\n",
    "#ff=plot_confusion_matrix(invtrans.detach().numpy()[0],['-','L','Q','R'],[0,1,2,3,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_exec_experiment(path_preexist_modelGP, theta_est_GP, maxiter, optimizerGP, T, x1 \\\n",
    "                            ,ref_msa ,modeflag, msa_numGP, loss_valsgp, loss_function, \\\n",
    "                            inverse = True, interp_inv = 'GP', interp_dir = 'linear'):\n",
    "\n",
    "    T.interpolation_type = interp_inv\n",
    "    theta_est_GP = training_theta_optima(path_preexist_modelGP, theta_est_GP, maxiter, optimizerGP, T, x1, ref_msa, \n",
    "                                                modeflag, msa_numGP, loss_valsgp, loss_function, inverse = inverse)\n",
    "\n",
    "    # Executing inverse transform from optimal theta and using GPs as interpolator\n",
    "    x1_trans3, sampled_data3, forw_per = T.spatial_transformation(x1, ref_msa, -theta_est_GP, modeflag)\n",
    "\n",
    "    # Executing direct transform from optimal theta and using linear as interpolator\n",
    "    T.interpolation_type = interp_dir\n",
    "    x1_trans3_dir, _, forw_per = T.spatial_transformation(x1_trans3, ref_msa, theta_est_GP, modeflag)\n",
    "\n",
    "    print(\"reverse transformed CPAB using GP: \\n\\n {0}\".format(x1_trans3))\n",
    "    xtrans_rev_logos = plot_logos_probs(x1_trans3, alphabets)\n",
    "    writer.add_text('inverse_trans1', str(x1_trans3.detach().numpy()))\n",
    "    print(\"direct transformed CPAB using linear with theta_GP: \\n\\n {0}\".format(x1_trans3_dir))\n",
    "    xtrans_dir_logos = plot_logos_probs(x1_trans3_dir, alphabets)\n",
    "    writer.add_text('direct_trans1',str( str(x1_trans3_dir.detach().numpy())))\n",
    "\n",
    "    # Plot logos in tensorboard\n",
    "    writer.add_figure('inverse_trans_Fig_LOGOS', xtrans_rev_logos.fig)\n",
    "    writer.add_figure('direct_trans_Fig_LOGOS', xtrans_dir_logos.fig)\n",
    "\n",
    "    # Plot components in grid, due to lack of support for writting tensors inside tensorboard\n",
    "    from torchvision.utils import make_grid\n",
    "    writer.add_figure('Optimal Inverse Transform - Forward (-,L,Q,R)', heatmap_from_tensor(x1_trans3[0].detach().numpy(), alphabets, 'Forward (-,L,Q,R)') )\n",
    "    writer.add_figure('Optimal Direct Transform - Backward (-,L,Q,R)', heatmap_from_tensor(x1_trans3_dir[0].detach().numpy(), alphabets, 'Backward (-,L,Q,R)') )\n",
    "\n",
    "    # Info about grid deformations:\n",
    "    grid_no_expansion = T.make_grids_for_Regresion(batch_size = x1.shape[0])\n",
    "    grid_t_no_expansion_rev = T.transform_grid(grid_no_expansion, -theta_est_GP)\n",
    "    grid_t_no_expansion_dir = T.transform_grid(grid_no_expansion, theta_est_GP)\n",
    "\n",
    "    forward = torch.stack([grid_no_expansion.flatten() ,grid_t_no_expansion_rev.flatten(), grid_t_no_expansion_rev.flatten()*(x1.shape[1]-1)]).T\n",
    "    backward = torch.stack([grid_no_expansion.flatten() ,grid_t_no_expansion_dir.flatten(), grid_t_no_expansion_dir.flatten()*(x1.shape[1]-1)]).T\n",
    "\n",
    "    writer.add_text('Inverse T Deformation RAW',str( str(grid_t_no_expansion_rev.detach().numpy())))\n",
    "    writer.add_text('Direct T Deformation RAW',str( str(grid_t_no_expansion_dir.detach().numpy())))\n",
    "    writer.add_text('Inverse T Deformation SCALED',str( str(grid_t_no_expansion_rev.detach().numpy()*(x1.shape[1]-1) )))\n",
    "    writer.add_text('Direct T Deformation SCALED',str( str(grid_t_no_expansion_dir .detach().numpy()*(x1.shape[1]-1) )))\n",
    "\n",
    "    y_labs = list(range(grid_no_expansion.shape[-1]))\n",
    "    plot_forward = heatmap_from_tensor(forward.detach().numpy(), ['input','raw','scaled'], 'FORWARD SCHEME' )\n",
    "    plot_backward = heatmap_from_tensor(backward.detach().numpy(), ['input','raw','scaled'], 'BACKWARD SCHEME' )\n",
    "    writer.add_figure('FORWARD',  plot_forward ) \n",
    "    writer.add_figure('BACKWARD',  plot_backward ) \n",
    "\n",
    "\n",
    "\n",
    "    return x1_trans3,x1_trans3_dir, theta_est_GP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/Explicit_Disentanglement_Proteins/src/gp_cpab/src/transformation/libcpab/libcpab/pytorch/functions.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(x, dtype=dtype, device=device)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'gp_cpab' object has no attribute 'get_interpolation_inductive_points'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 16\u001b[0m\n\u001b[1;32m     11\u001b[0m fill_criteria \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtoken,token_target\u001b[39m\u001b[39m'\u001b[39m \u001b[39m#'token,gap'\u001b[39;00m\n\u001b[1;32m     12\u001b[0m init_theta \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m \u001b[39m#True #True #False #True\u001b[39;00m\n\u001b[1;32m     14\u001b[0m alphabets, c2i, i2c, i2i, dataset_msa, \\\n\u001b[1;32m     15\u001b[0m padded_idx, non_padded_idx, x1, ref_msa,\\\n\u001b[0;32m---> 16\u001b[0m      outsize, T \u001b[39m=\u001b[39m define_data_and_transformations(path_MSA_test, path, gp_params, option, T, type_of_fill\u001b[39m=\u001b[39;49mfill_criteria, length_filling \u001b[39m=\u001b[39;49m \u001b[39m0\u001b[39;49m, alphabet \u001b[39m=\u001b[39;49m [\u001b[39m'\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mL\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mQ\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mR\u001b[39;49m\u001b[39m'\u001b[39;49m]) \u001b[39m#'token', 'gap'\u001b[39;00m\n\u001b[1;32m     19\u001b[0m theta_ref, theta_est, theta_est_GP \u001b[39m=\u001b[39m define_thetas(init\u001b[39m=\u001b[39minit_theta)\n\u001b[1;32m     21\u001b[0m lr, wd, maxiter, optimizer, optimizerGP, loss_function \u001b[39m=\u001b[39m optimization_setup(std, theta_est, theta_est_GP, c2i, i2c, i2i, [\u001b[39m'\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mweight_decay\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mmaxiter\u001b[39m\u001b[39m'\u001b[39m])\n",
      "Cell \u001b[0;32mIn[37], line 24\u001b[0m, in \u001b[0;36mdefine_data_and_transformations\u001b[0;34m(path_MSA_test, path, gp_params, option, T, type_of_fill, length_filling, **kargs)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[39m''' LIKELIHOOD DEFINITIONS:'''\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[39mif\u001b[39;00m option \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mmultitask\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m---> 24\u001b[0m     T\u001b[39m.\u001b[39;49mget_interpolation_inductive_points(x1, x1\u001b[39m.\u001b[39mfloat(), \n\u001b[1;32m     25\u001b[0m                                         outsize \u001b[39m=\u001b[39m outsize, \n\u001b[1;32m     26\u001b[0m                                         padded_idx \u001b[39m=\u001b[39m padded_idx, \n\u001b[1;32m     27\u001b[0m                                         non_padded_idx \u001b[39m=\u001b[39m non_padded_idx,\n\u001b[1;32m     28\u001b[0m                                         \u001b[39m#separation_size_grid = T.params.inc[0],\u001b[39;00m\n\u001b[1;32m     29\u001b[0m                                         padding_option \u001b[39m=\u001b[39m padding_criteria  )\n\u001b[1;32m     30\u001b[0m                                         \u001b[39m#padding_option = 'none'   )\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[39mreturn\u001b[39;00m alphabets, c2i, i2c, i2i, dataset_msa, padded_idx, non_padded_idx, x1, ref_msa, outsize, T\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'gp_cpab' object has no attribute 'get_interpolation_inductive_points'"
     ]
    }
   ],
   "source": [
    "'''Transformations for Reference Alignment'''\n",
    "'''-----------------------------------------------------------------------------------------------'''\n",
    "'''      JUST FOR CAPTURING DEFORMATION PATTERNS ALONG THE CPAB, HOW IT STARS AND IT ENDS:        '''\n",
    "'''-----------------------------------------------------------------------------------------------'''\n",
    "''' NEEDS TO BE IN ZERO_BOUNDARY IN FALSE FOR GETTING PROPER RESULTS IN ALIGNMENTS'''\n",
    "from transformation.gp_cpab import gp_cpab\n",
    "ndim = [window_grid]\n",
    "T = gp_cpab(ndim, std, backend='pytorch', device=device, zero_boundary=False,\n",
    "                                          volume_perservation=False, override=False, argparser_gpdata = gp_params)\n",
    "\n",
    "fill_criteria = 'token,token_target' #'token,gap'\n",
    "init_theta = True #True #True #False #True\n",
    "\n",
    "alphabets, c2i, i2c, i2i, dataset_msa, \\\n",
    "padded_idx, non_padded_idx, x1, ref_msa,\\\n",
    "     outsize, T = define_data_and_transformations(path_MSA_test, path, gp_params, option, T, type_of_fill=fill_criteria, length_filling = 0, alphabet = ['-','.', 'L', 'Q', 'R']) #'token', 'gap'\n",
    "\n",
    "\n",
    "theta_ref, theta_est, theta_est_GP = define_thetas(init=init_theta)\n",
    "\n",
    "lr, wd, maxiter, optimizer, optimizerGP, loss_function = optimization_setup(std, theta_est, theta_est_GP, c2i, i2c, i2i, ['lr','weight_decay','maxiter'])\n",
    "\n",
    "msa_num = []; loss_vals =[]\n",
    "msa_numGP = []; loss_valsgp =[]\n",
    "\n",
    "x1plot = plot_logos_probs(x1, alphabets)\n",
    "refplot = plot_logos_probs(ref_msa, alphabets)\n",
    "\n",
    "# Better with GP-GP configuration, instead of GP-Linear for inverse-reverse, why?\n",
    "invtrans,dirtrans, theta_est_GP = training_exec_experiment(path_preexist_modelGP, theta_est_GP, maxiter, optimizerGP, T, x1 \\\n",
    "                            ,ref_msa ,modeflag, msa_numGP, loss_valsgp, loss_function, \\\n",
    "                            inverse = True, interp_inv = 'GP', interp_dir = 'GP') #linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'theta_est_GP' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mprint\u001b[39m(theta_est_GP)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'theta_est_GP' is not defined"
     ]
    }
   ],
   "source": [
    "print(theta_est_GP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "performing iteration 0: \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "fp and xp are not of the same length.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 42\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mgrid values after theta:  \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(res[\u001b[39m1\u001b[39m]))\n\u001b[1;32m     39\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mloss value:  \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(res[\u001b[39m2\u001b[39m]))\n\u001b[0;32m---> 42\u001b[0m landscape_analysis(theta_est_GP, Optimal_theta, ref_msa, writer_land, \u001b[39m55\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[26], line 25\u001b[0m, in \u001b[0;36mlandscape_analysis\u001b[0;34m(init_theta, opt_theta, target_val, Wtboard, times)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mfor\u001b[39;00m cont,i \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(swept_grid):\n\u001b[1;32m     24\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mperforming iteration \u001b[39m\u001b[39m{\u001b[39;00mcont\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 25\u001b[0m     tmp_theta \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49minterp(i, T_opt_grid\u001b[39m.\u001b[39;49mflatten()\u001b[39m.\u001b[39;49mdetach()\u001b[39m.\u001b[39;49mnumpy(), \u001b[39m-\u001b[39;49mopt_theta\u001b[39m.\u001b[39;49mflatten()\u001b[39m.\u001b[39;49mdetach()\u001b[39m.\u001b[39;49mnumpy())\u001b[39m.\u001b[39mreshape(\u001b[39m1\u001b[39m,\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     26\u001b[0m     tmp_theta \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor( tmp_theta)\u001b[39m.\u001b[39mfloat()\n\u001b[1;32m     27\u001b[0m     x1_trans, sampled_data, forw_per \u001b[39m=\u001b[39m T\u001b[39m.\u001b[39mspatial_transformation(x1, ref_msa, tmp_theta, modeflag)\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36minterp\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/numpy/lib/function_base.py:1570\u001b[0m, in \u001b[0;36minterp\u001b[0;34m(x, xp, fp, left, right, period)\u001b[0m\n\u001b[1;32m   1567\u001b[0m     xp \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mconcatenate((xp[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:]\u001b[39m-\u001b[39mperiod, xp, xp[\u001b[39m0\u001b[39m:\u001b[39m1\u001b[39m]\u001b[39m+\u001b[39mperiod))\n\u001b[1;32m   1568\u001b[0m     fp \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mconcatenate((fp[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:], fp, fp[\u001b[39m0\u001b[39m:\u001b[39m1\u001b[39m]))\n\u001b[0;32m-> 1570\u001b[0m \u001b[39mreturn\u001b[39;00m interp_func(x, xp, fp, left, right)\n",
      "\u001b[0;31mValueError\u001b[0m: fp and xp are not of the same length."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "logdir_landscape = args.logdir + '/res/' + 'parameters_landscape'\n",
    "  \n",
    "writer_land = SummaryWriter(log_dir=logdir_landscape)\n",
    "\n",
    "default_theta = torch.autograd.Variable(T.identity(1, epsilon=1e-6), requires_grad=True)\n",
    "init_theta = torch.tensor([[0.0011, 0.019, -0.119, -0.015356, 0.952]],requires_grad=True)\n",
    "Optimal_theta = torch.tensor([[-1.0655,  0.5902,  0.1982, -0.5455, -1.1456]], requires_grad=True)\n",
    "SubOpt_theta_when_False = theta_est_GP#torch.tensor([[-0.6950,  1.8791, -0.9911,  2.5290, -0.5128]], requires_grad=True)\n",
    "\n",
    "def landscape_analysis(init_theta, opt_theta, target_val, Wtboard, times=20):\n",
    "    Ugrid = T.uniform_meshgrid(outsize).repeat(1,1,1)\n",
    "    T_init_grid = T.transform_grid(Ugrid, -init_theta)\n",
    "    T_opt_grid = T.transform_grid(Ugrid, -opt_theta)\n",
    "    #times = 10 #500\n",
    "\n",
    "    swept_grid = [ np.linspace( T_init_grid[:,:,i].detach().numpy(), T_opt_grid[:,:,i].detach().numpy(), times-1).flatten() for i in range(0,len(Ugrid.flatten())) ]\n",
    "    swept_grid = np.array(swept_grid).T\n",
    "    results = []; loss_evolution = []\n",
    "\n",
    "    #cont=0\n",
    "    for cont,i in enumerate(swept_grid):\n",
    "        print(f'performing iteration {cont}: ')\n",
    "        tmp_theta = np.interp(i, T_opt_grid.flatten().detach().numpy(), -opt_theta.flatten().detach().numpy()).reshape(1,-1)\n",
    "        tmp_theta = torch.tensor( tmp_theta).float()\n",
    "        x1_trans, sampled_data, forw_per = T.spatial_transformation(x1, ref_msa, tmp_theta, modeflag)\n",
    "        loss = loss_function(method = lossmetric, input = x1_trans, target = target_val, forw_per=forw_per)\n",
    "        results.append( (tmp_theta, T.transform_grid(Ugrid, tmp_theta), loss) )\n",
    "        loss_evolution.append(loss.item())\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.plot(list(range(0,times-1)),loss_evolution)\n",
    "\n",
    "    for res in results:\n",
    "        #print(\"for position {0}, this is the information: \\n\\n\".format(i) )\n",
    "        print(\"theta:  {0}\".format(res[0]))\n",
    "        print(\"grid values after theta:  {0}\".format(res[1]))\n",
    "        print(\"loss value:  {0} \\n\\n\".format(res[2]))\n",
    "\n",
    "\n",
    "landscape_analysis(theta_est_GP, Optimal_theta, ref_msa, writer_land, 55)\n",
    "#landscape_analysis(init_theta, Optimal_theta, ref_msa, writer_land, 55)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
